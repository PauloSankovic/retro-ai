\chapter{Implementacija}

Kompletna implementacija napisana je u programskom jeziku \textit{Python} koji je odabran zbog svoje jednostavnosti, sažetosti, fleksibilnosti, neovisnosti o operacijskom sustavu, te velikoj popularnosti. Postoji veliki broj dobro dokumentiranih i iznimno korisnih biblioteka od kojih se za problematiku ovog rada izdvajaju biblioteke za oblikovanje i učenje dubokih modela (\textit{PyTorch}), biblioteka za iznimno brzo i učinkovito provođenje matematičkih operacija (\textit{Numpy}), te biblioteka za jednostavnu interakciju agenata i implementiranih okolina s kojima agent interaktira i dobiva povratnu informaciju (\textit{OpenAI Gym}).

Implementirani su duboki modeli prethodno predstavljenih algoritma dubokog Q učenja, dvostrukog dubokog Q učenja, te prednosnog akter-kritičara. Agenti u svojoj implementaciji interaktiraju s okolinama CartPole i Breakout.

\section{Moduli}

Implementacija je podijeljena u nekoliko modula: modul za generičko instanciranje dubokih modela, modul za generičku serijalizaciju i deserijalizaciju objekata (spremanje i učitavanje agenata), modul za logiranje, modul za \textit{OpenAI Gym} omotače i modul u kojem se nalazi implementacija agenata i algoritama.

\subsection{Instanciranje dubokih modela}

Modul \texttt{networks} sastoji se od metoda i struktura koje omogućuju generičko instanciranje dubokih modela, točnije unaprijednih potpuno povezanih modela i konvolucijskih modela opisanih u poglavlju \nameref{chap:duboki-modeli}. Prilikom poziva metode za instanciranje unaprijedne potpuno povezane mreže kao argument predaje se lista cijelih brojeva koje predstavljaju dimenzije pojedinih slojeva odvojenih nelinearnom aktivacijskom funkcijom ReLU (kao što je prikazano odsječkom \ref{lst:custom-fc}).

\begin{listing}[H]
    \caption{Generičko instanciranje unaprijedne potpuno povezane mreže}
    \inputminted{python}{snippets/custom-fc.py}
    \label{lst:custom-fc}
\end{listing}

S druge strane, pri instanciranju konvolucijske neuronske mreže, za definiranje atributa konvolucijskog sloja koristi se posebna struktura \texttt{CnnStructure} \ref{lst:cnn-structures} koja definira broj ulaznih i izlaznih kanala, dimenziju jezgre (slobodnih parametara koje učimo), veličinu koraka i nadopunu. Nakon konvolucijskog sloja provodi se sažimanje maksimumom (pritom se smanjuje broj značajki) i provodi nelinearnost aktivacijskom funkcijom ReLU. Na poslijetku, značajke se transformiraju u vektor i prosljeđuju potpuno povezanom sloju. TODO određivanje broja parametara!!!! Generični poziv funkcije za instanciranje konvolucijske neuronske mreže prikazan je odsječkom \ref{custom-cnn}.

\begin{listing}[H]
    \caption{Generičko instanciranje unaprijedne potpuno povezane mreže}
    \inputminted{python}{snippets/structures.py}
    \label{lst:cnn-structures}
\end{listing}

\subsection{Serijalizacija i deserijalizacija objekata}

\subsection{Logiranje}

\subsection{Omotači}





\section{Okolina CartPole}

\subsection{Implementacija Deep Q Learning algoritma}

\url{https://github.com/PauloSankovic/retro-ai/blob/master/models/DeepQNetworkAgent-state_env\%3Dcartpole_gamma\%3D0.99_lr\%3D5e-4_bs\%3D32_es\%3D1_ee\%3D0.02.pickle}

\subsection{Implementacija Double Deep Q Learning algoritma}

\subsection{Implementacija Actor Critic algoritma}

\url{https://github.com/PauloSankovic/retro-ai/blob/master/models/ActorCriticAgent_env\%3Dcarpole_gamma\%3D0.99_lr\%3D3e-2.pickle}

video igranja:
\url{https://raw.githubusercontent.com/PauloSankovic/retro-ai/master/playground/video/cart-pole/openaigym.video.0.9768.video000000.mp4}

\subsection{Usporedba}

\section{Okolina Breakout}

\subsection{Implementacija Double Deep Q Learning algoritma}

\url{https://github.com/PauloSankovic/retro-ai/blob/master/models/DoubleDeepQNetworkAgent-state_env\%3Dbreakout_step\%3D1400000_v\%3D2.pickle}

video igranja:
\url{https://github.com/PauloSankovic/retro-ai/tree/master/playground/video/breakout-ddqn}

\subsection{Implementacija Deep Q Learning algoritma}

\url{https://github.com/PauloSankovic/retro-ai/blob/master/models/DeepQNetworkAgent-state_env\%3Dbreakout_step\%3D1080000.pickle}

\subsection{Implementacija Actor Critic algoritma}

Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.

To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).

\subsection{Usporedba}

\section{Usporedba algoritama}

