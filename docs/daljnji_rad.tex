\chapter{Daljnji rad}

U prethodnim poglavljima predstavljeni su i implementirani pojedini algoritmi dubokog podržanog učenja koristeći programski jezik \textit{Python}, te primarno biblioteku \textit{PyTorch} za oblikovanje i učenje dubokih modela, te okoline biblioteke \textit{OpenAI Gym} u kojima je agent bio smješten i s kojima je interaktirao i dobivao povratne informacije. Osim obrađenih algoritama dubokog Q učenja, dvostrukog dubokog Q učenja, akter-kritičara, te prednosnog akter-kritičara, u daljnjem radu bilo bi korisno predstaviti, implementirati i isprobati algoritme poput suparničkog dvostrukog Q učenja \engl{Dueling Double Deep Q Network} (skraćeno \textit{D3QN}), asinkronog prednosnog akter-kritičara \engl{Asynchronous Advantage Actor-Critic} (skraćeno \textit{A3C}), mekog akter-kritičara \engl{Soft Actor-Critic} (skraćeno \textit{SAC}), Trust Region Policy Optimization (skraćeno \textit{TRPO}), Proximal Policy Optimization (skraćeno \textit{PPO}) koji u određenim slučajevima daju bolje rezultate od obrađenih algoritama.

Pronalazak optimalnih hiperparametara dubokih modela ovisnima o okolini u kojoj se nalaze je veoma iscrpan i vremenski zahtjevan postupak, no ispravan odabir hiperparametara kao i arhitekture dubokog modela, može uvelike poboljšati performanse agenta. Osim hiperparametara, performanse dubokog modela može poboljšati i korištenje različitih optimizacijskih postupaka. Prilikom implementacije koristio se optimizacijski postupak Adam \engl{Adaptive Moments}, no moguće je koristiti i obični stohastički gradijentni spust \engl{Stohastic Gradient Descent} (skraćeno \textit{SGD}), AdaGrad ili RMSProp u kombinaciji s momentom i restartom kod promjene stope učenja. Također, bilo bi korisno pri prolasku podataka kroz konvolucijske slojeve primijeniti sažimanje maksimumom. Ono u obzir uzima samo značajku najveće vrijednosti te na taj način uklanja šum ulaza i potencijalno bira značajku najveće važnosti. Korisno bi bilo i nakon određenog broja iteracija učenja provesti validacijsku epizodu nad trenutnim stanjem agenta (bez ikakvih omotača) i na taj način procijeniti koliko se ponašanje u međuvremenu poboljšalo ili pogoršalo.

Također, u daljnjem radu korisno bi bilo smjestiti agente te provjeriti kako se ponašaju u novim okolinama i suočavaju s njihovom problematikom. Posebno se zanimljivima čine \textit{OpenAI Gym} okoline \textit{Double Dunk}, \textit{Space Invaders}, \textit{Car Racing}, \textit{Pong}, te cijela porodica \textit{MuJoCo} okolina za simulaciju i kontrolu. \textit{OpenAI Gym} ima podršku za jednostavan razvoj i ručnu implementaciju novih igara i njihovih okolina, poput igrica \textit{Flappy Bird} \cite{CustomFlappyBird} ili \textit{Dino Game} \cite{CustomDinoGame}.

Podržano učenje ima veliku primjenu i u trgovanju dionicama, valutama \engl{forex}, kriptovalutama za čije bi se treniranje agenata i interakciju mogle koristiti biblioteke \textit{Gym AnyTrading} (slika \ref{fig:gym-anytrading}) \cite{GymAnytrading} i \textit{TensorTrade} \cite{Tensortrade}.

\begin{figure}[H]
    \centering
    \frame{\includegraphics[width=8cm]{assets/gym-anytrading.png}}
    \caption{\textit{Gym AnyTrading} okolina s označenim kupovnim i prodajnim pozicijama}
    \label{fig:gym-anytrading}
\end{figure}
