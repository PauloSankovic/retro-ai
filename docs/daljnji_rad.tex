\chapter{Daljnji rad}

U prethodnim poglavljima predstavili su se osnovni algoritmi dubokog podržanog učenja implementirani dubokim modelima u \textit{Python} programskom jeziku, primarno koristeći biblioteke \textit{PyTorch} i \textit{OpenAI Gym}. Osim obrađenih algoritama: duboko Q učenje, dvostruko duboko Q učenje, akter-kritičar, te prednosni akter-kritičar, korisno bi u budućnosti predstaviti i implementirati algoritme poput dvostruko...

Također, korisno bi bilo i naučiti agente te provjeriti kako se ponašaju na novim vrstama okolina i kako se suočavaju s novim problemima. Posebno se zanimljivima čine \textit{OpenAI Gym} okoline \textit{Double Dunk} \cite{https://www.gymlibrary.ml/environments/atari/double_dunk/}, \textit{Space Invaders} \cite{https://www.gymlibrary.ml/environments/atari/space_invaders/}, \textit{Car Racing} \cite{https://www.gymlibrary.ml/environments/box2d/car_racing/}, te cijela porodica \textit{MuJoCo} okolina za simulaciju i kontrolu. Podržano učenje ima veliku primjenu i u trgovanju dionicama, valutama, kriptovalutama za čije bi se treniranje agenata i interakciju mogle koristiti \textit{Gym AnyTrading} \cite{https://github.com/AminHP/gym-anytrading} i \textit{TensorTrade} \cite{https://github.com/tensortrade-org/tensortrade} okoline.

\textit{OpenAI Gym} ima podršku za jednostavan razvoj i implementaciju novih okolina. 

\bigskip

Korištenje drugih optimzacijskih postupaka osim Adam-a kao npr. SGD, SGD s momentom, SGD s restartom, RMSProp, RMSProp s momentom
Korištenje restarta kod promjene stope učenja... igranje s hiperparametrima

Isprobati više verzija arhitekture dubokog modela i procijeniti benefite svakog.
